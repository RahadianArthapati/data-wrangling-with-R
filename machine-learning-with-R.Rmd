---
title: "machine-learning-1"
author: "Rahadian Arthapati"
date: "10/26/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Tidy Text
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

glimpse(original_books)
```
To work with this as a tidy dataset, we need to restructure it as one-token-per-row format. The unnest_tokens function is a way to convert a dataframe with a text column to be one-token-per-row
```{r}
library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)

glimpse(tidy_books)
```
Now that the data is in one-word-per-row format, we can manipulate it with tidy tools like dplyr. We can remove stop words (kept in the tidytext dataset stop_words) with an anti_join. We can also use count to find the most common words in all the books as a whole.
```{r}
data("stop_words")
cleaned_books <- tidy_books %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) 
```
Three sentiment lexicons are in the tidytext package in the sentiments dataset. Let’s look at the words with a joy score from the NRC lexicon. What are the most common joy words in Emma?
```{r}
nrcjoy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  semi_join(nrcjoy) %>%
  count(word, sort = TRUE)
```
Or instead we could examine how sentiment changes during each novel. Let’s find a sentiment score for each word using the Bing lexicon, then count the number of positive and negative words in defined sections of each novel.
```{r}
bing <- get_sentiments("bing")
janeaustensentiment <- tidy_books %>%
  inner_join(bing) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```
Let's plot these sentiment scores
```{r}
library(ggplot2)

ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

##Challenge 1
#Spooky Author Identification
#https://www.kaggle.com/c/spooky-author-identification
```{r}
library(tidytext)
library(tidyverse)
library(glue)

texts<-read.csv("dataset/spooky/train.csv")
glimpse(texts)
```

```{r}
# split the data by author
byAuthor <- train%>%group_by(author)
### Calcuate how often each author uses each word

freqByAuthor <-  texts %>%
    group_by(author) %>% # group by author
    select(text) %>% # grab only the Sentence column
    mutate(text = as.character(text)) %>% # convert them to characters
    unnest_tokens(words, text) %>% # tokenize
    count(words) %>% # frequency by token (by author)
    bind_tf_idf(words, author, n) # normalized frequency
```
```{r}
### Look at how often each writer uses specific words

# see how often each author says "blood"
print(freqByAuthor[freqByAuthor$words == "blood",])

# see how often each author says "scream"
print(freqByAuthor[freqByAuthor$words == "scream",])

# see how often each author says "fear"
print(freqByAuthor[freqByAuthor$words == "fear",])
```

